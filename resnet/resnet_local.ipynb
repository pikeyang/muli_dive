{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channel, num_channel,\n",
    "                 use_1x1conv=False, stride=1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channel, out_channels=num_channel,\n",
    "                               kernel_size=3, padding=1, stride=stride)  # use stride to down sampling, this stride correspond to conv3's stride\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_channel, out_channels=num_channel,\n",
    "                               kernel_size=3, padding=1)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels=input_channel, out_channels=num_channel,\n",
    "                                   kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(num_channel)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channel)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(input_channel=3, num_channel=3)\n",
    "X = torch.randn(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(input_channel=3, num_channel=6, use_1x1conv=True, stride=2)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def resnet_block(input_channel, num_channel, num_residual, first_block=False):\n",
    "    blk = []\n",
    "\n",
    "    for i in range(num_residual):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channel, num_channel, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channel, num_channel))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, num_residual=2, first_block=True))  # without 1x1\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, num_residual=2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, num_residual=2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, num_residual=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                    nn.Flatten(), nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, root='./dataset/FashionMNIST'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True,\n",
    "                                                    download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False,\n",
    "                                                   download=True, transform=transform)\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 64\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device\n",
    "\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum()\n",
    "                net.train()\n",
    "            else:  # 自定义的模型, 3.13节之后不会用到, 不考虑GPU\n",
    "                if 'is_training' in net.__code__.co_varnames:  # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()\n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        #  Modifying param with param.data will not be passed to the calculation diagram\n",
    "        param.data -= lr * param.grad / batch_size\n",
    "\n",
    "def train_func(net: nn.Module, train_iter, test_iter, loss, num_epoch: int, batch_size: int, device,\n",
    "               params=None, lr=None,\n",
    "               optimizer=None,\n",
    "               writer=None):\n",
    "    \"\"\"\n",
    "    :param net:\n",
    "    :param train_iter:\n",
    "    :param test_iter:\n",
    "    :param loss:\n",
    "    :param num_epoch:\n",
    "    :param batch_size:\n",
    "    :param device:\n",
    "    :param params:\n",
    "    :param lr:\n",
    "    :param optimizer:\n",
    "    :param writer: for tensorboard\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "\n",
    "    if loss is None:\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for batch_count, (X, y) in enumerate(train_iter):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            loss_fn = loss(y_hat, y)\n",
    "\n",
    "            if optimizer is not None:  # use module\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            loss_fn.backward()\n",
    "\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss_fn.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).cpu().sum().item()\n",
    "            n += y.shape[0]\n",
    "\n",
    "            if writer is not None:\n",
    "                if batch_count % 100 == 99:\n",
    "                    writer.add_scalar('train_loss',\n",
    "                                      train_loss_sum / n,\n",
    "                                      epoch * len(train_iter) + batch_count)\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_loss_sum / n, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.0157, train acc 0.708, test acc 0.818, time 141.3 sec\n",
      "epoch 2, loss 0.0071, train acc 0.834, test acc 0.837, time 148.3 sec\n",
      "epoch 3, loss 0.0061, train acc 0.855, test acc 0.808, time 145.1 sec\n",
      "epoch 4, loss 0.0055, train acc 0.871, test acc 0.845, time 126.2 sec\n",
      "epoch 5, loss 0.0051, train acc 0.881, test acc 0.862, time 127.0 sec\n",
      "epoch 6, loss 0.0048, train acc 0.890, test acc 0.854, time 128.0 sec\n",
      "epoch 7, loss 0.0045, train acc 0.894, test acc 0.886, time 128.7 sec\n",
      "epoch 8, loss 0.0043, train acc 0.900, test acc 0.877, time 129.8 sec\n",
      "epoch 9, loss 0.0041, train acc 0.903, test acc 0.890, time 124.9 sec\n",
      "epoch 10, loss 0.0040, train acc 0.907, test acc 0.833, time 127.2 sec\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "train_func(net, train_iter, test_iter, None, num_epochs, batch_size, device, lr=lr, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_layer = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "X = torch.randn((1, 1, 224, 224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 64, 112, 112])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_layer(X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}